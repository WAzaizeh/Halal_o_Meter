{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, std\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(file_path):\n",
    "    # load the dataset as a dataframe\n",
    "    data_df = pd.read_pickle(file_path)\n",
    "    # correct data types\n",
    "    fix_cols = data_df.columns[data_df.dtypes == 'object']\n",
    "    data_df.loc[:, fix_cols] = data_df.loc[:, fix_cols].astype('bool')\n",
    "    # remove redundant features\n",
    "    data_df.drop(['total_review_count', 'halal_review_count', 'halal_negation_count'], inplace=True, axis=1)\n",
    "    # split into features and target\n",
    "    X, y = data_df.drop('halal', axis=1), data_df.halal\n",
    "    return X, y\n",
    "\n",
    "# define resampling method\n",
    "def split_and_resample(X, y, test_size=0, resampling=None):\n",
    "    if test_size > 0:\n",
    "        # setting up testing and training sets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = X, pd.DataFrame(columns=X.columns, dtype=X.dtypes.values),\\\n",
    "                                            y, pd.Series(name=y.name, dtype=y.dtypes)\n",
    "    # concatenate our training data back together\n",
    "    X = pd.concat([x_train, y_train], axis=1)\n",
    "    # separate minority and majority classes\n",
    "    min_class = X[X.halal==X.halal.value_counts().sort_values().index[0]]\n",
    "    max_class = X[X.halal==X.halal.value_counts().sort_values().index[1]]\n",
    "    if not resampling:\n",
    "        return x_train, y_train, x_test, y_test\n",
    "    # oversampling minority\n",
    "    if resampling == 'oversample':\n",
    "        new_min_class = resample(min_class, replace=True, n_samples=(len(max_class)))\n",
    "        upsampled = pd.concat([max_class, new_min_class])\n",
    "        return upsampled.drop('halal', axis=1), upsampled.halal, x_test, y_test\n",
    "    elif resampling == 'undersample':\n",
    "        new_max_class = resample(max_class, replace=True, n_samples=(len(min_class)))\n",
    "        downsampled = pd.concat([new_max_class, min_class])\n",
    "        return downsampled.drop('halal', axis=1), downsampled.halal, x_test, y_test\n",
    "    elif resampling == 'SMOTE':\n",
    "        x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model, n_splits=10):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=3)\n",
    "    # define scoring metrics\n",
    "    metrics = {'acc': 'accuracy', 'pre': 'precision', 'rec': 'recall', 'f1': 'f1_macro'}\n",
    "    # evaluate model\n",
    "    scores = cross_validate(model, X, y, scoring=metrics, cv=cv)\n",
    "    return scores\n",
    "\n",
    "# calculate precision-recall area under curve\n",
    "def pr_auc(y_true, probas_pred):\n",
    "   # calculate precision-recall curve\n",
    "   p, r, _ = precision_recall_curve(y_true, probas_pred)\n",
    "   # calculate area under curve\n",
    "   return auc(r, p)\n",
    "\n",
    "# evaluate a model\n",
    "def pr_auc_score(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "    # define the model evaluation the metric\n",
    "    metric = make_scorer(pr_auc, needs_proba=True)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# evaluate a model\n",
    "def cross_validate_f1_macroe(X, y, model, n_splits=10):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=3)\n",
    "    # evaluate model\n",
    "    scores = cross_validate(model, X, y, scoring='f1_macro', cv=cv)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average of percent of halal reviews, count of relevant halal reviews, and count of mentions of halal bacon is higher for halal resaurants than non-halal restaurants as expected.\n",
    "- The average of non-halal relevant reviews and percent of reviews that include halal-negation is higher in non-halal restaurants as expected.\n",
    "- The count of halal-truck mentions doesn't seem significantly different between the two groups. This feature randked 7th in RF so there could be information that's not reflected by the groups averages in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logistic_reg():\n",
    "    # define the location of the dataset\n",
    "    file_path = '/Users/wesamazaizeh/Desktop/Projects/halal_o_meter/src/features/feature_engineering/restaurant_cat_and_num_v3.pkl'\n",
    "    # load the dataset\n",
    "    X, y = load_dataset(file_path)\n",
    "    # transform bool target to 1/0\n",
    "    y = y*1\n",
    "    # select numerical columns\n",
    "    X_keep = X.columns[X.dtypes == 'float64']\n",
    "    X_num = X.loc[:, X_keep]\n",
    "    # class observations\n",
    "    data = pd.concat([X_num, y], axis=1)\n",
    "    # create dummy features from categorial features\n",
    "    cat_cols = X.columns[X.dtypes == 'bool']\n",
    "    dummy_X = pd.concat([pd.get_dummies(X[col], prefix=col) for col in cat_cols], axis=1)\n",
    "    # drop categorial features and replace with dummy features\n",
    "    X = pd.concat([X, dummy_X], axis=1)\n",
    "    X.drop(cat_cols, axis=1, inplace=True)\n",
    "    # split to train and test\n",
    "    X_train, y_train, X_test, y_test = split_and_resample(X, y, test_size=0.2)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.763 +/- 0.039\n",
      "Precision: 0.799 +/- 0.032\n",
      "Recall: 0.904 +/- 0.042\n",
      "f1: 0.652 +/- 0.068\n",
      "Confusion matrix:\n",
      " [[ 20  24]\n",
      " [  7 117]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.45      0.56        44\n",
      "           1       0.83      0.94      0.88       124\n",
      "\n",
      "    accuracy                           0.82       168\n",
      "   macro avg       0.79      0.70      0.72       168\n",
      "weighted avg       0.81      0.82      0.80       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data, convert to proper format and split to train and test\n",
    "X_train, y_train, X_test, y_test = start_logistic_reg()\n",
    "\n",
    "# define default logistic regressor\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# evaluate default logistic regressor\n",
    "cv_scores = evaluate_model(X_train, y_train, logreg)\n",
    "cv_scores\n",
    "print('Accuracy: {:.3f} +/- {:.3f}'.format(cv_scores['test_acc'].mean(), cv_scores['test_acc'].std()))\n",
    "print('Precision: {:.3f} +/- {:.3f}'.format(cv_scores['test_pre'].mean(), cv_scores['test_pre'].std()))\n",
    "print('Recall: {:.3f} +/- {:.3f}'.format(cv_scores['test_rec'].mean(), cv_scores['test_rec'].std()))\n",
    "print('f1: {:.3f} +/- {:.3f}'.format(cv_scores['test_f1'].mean(), cv_scores['test_f1'].std()))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('Classification report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add imbalance term (class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set PR-AUC : 0.873 +/- 0.084\n",
      "Accuracy : 0.744\n",
      "Precision : 0.737\n",
      "Recall : 0.983\n",
      "f1 : 0.580\n",
      "Cofusion matrix:\n",
      " [[ 10  41]\n",
      " [  2 115]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.20      0.32        51\n",
      "           1       0.74      0.98      0.84       117\n",
      "\n",
      "    accuracy                           0.74       168\n",
      "   macro avg       0.79      0.59      0.58       168\n",
      "weighted avg       0.77      0.74      0.68       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data, convert to proper format and split to train and test\n",
    "X_train, y_train, X_test, y_test = start_logistic_reg()\n",
    "\n",
    "# define class weights\n",
    "w = {0:26, 1:74}\n",
    "\n",
    "# define custom logistic regression\n",
    "logreg2 = LogisticRegression(class_weight=w, max_iter=1000)\n",
    "logreg2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predict and print performance summary\n",
    "y_pred = logreg2.predict(X_test)\n",
    "PR_auc = pr_auc_score(X_test, y_test, logreg2)\n",
    "print('Train set PR-AUC : {:.3f} +/- {:.3f}'.format(PR_auc.mean(), PR_auc.std()))\n",
    "print('Accuracy : {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Precision : {:.3f}'.format(precision_score(y_test, y_pred)))\n",
    "print('Recall : {:.3f}'.format(recall_score(y_test, y_pred)))\n",
    "print('f1 : {:.3f}'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "print('Cofusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('Classification report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline f1: 0.651 +/- (0.036)\n",
      "Top 10 interactions: [('halal_negation_percent', 'halal_burger_False', 0.672), ('halal_relevant_count', 'halal_lamb_True', 0.671), ('is_halal_count', 'halal_relevant_True', 0.67), ('halal_lamb_count', 'halal_in_name_False', 0.669), ('non_halal_relevant_False', 'halal_beef_False', 0.668), ('halal_lamb_False', 'halal_chicken_True', 0.668), ('halal_lamb_True', 'halal_burger_True', 0.668), ('is_halal_False', 'halal_meat_True', 0.668), ('is_halal_True', 'halal_goat_True', 0.668), ('partial_halal_False', 'is_halal_False', 0.668)]\n"
     ]
    }
   ],
   "source": [
    "# load data, convert to proper format and split to train and test\n",
    "X_train, y_train, X_test, y_test = start_logistic_reg()\n",
    "\n",
    "# define default logistic regressor\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# baseline f1_macro score for comparison\n",
    "baseline_scores = cross_validate_f1_macroe(X_train, y_train, logreg, n_splits=3)['test_score']\n",
    "baseline = np.mean(baseline_scores)\n",
    "\n",
    "interactions = []\n",
    "c=0\n",
    "for feature_A in X_train.columns:\n",
    "    for feature_B in X_train.columns:\n",
    "        if feature_A > feature_B:\n",
    "            X_train['interaction'] = X_train[feature_A] * X_train[feature_B]\n",
    "            score = np.mean(cross_validate_f1_macroe(X_train, y_train, logreg, n_splits=3)['test_score'])\n",
    "            if score > baseline:\n",
    "                interactions.append((feature_A, feature_B, round(score,3)))\n",
    "    print('[{0}/{1}]'.format(c, X_train.shape[1]), end='\\r', flush=True)\n",
    "    c+=1\n",
    "print('Baseline f1: {:.3f} +/- ({:.3f})'.format(baseline, np.std(baseline_scores)))\n",
    "print('Top 10 interactions: %s' % sorted(interactions, key=lambda x: x[2], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There doesn't seem to be a significant improvement from adding interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
