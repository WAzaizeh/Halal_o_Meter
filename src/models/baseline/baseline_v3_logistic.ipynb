{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, std\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(file_path):\n",
    "    # load the dataset as a dataframe\n",
    "    data_df = pd.read_pickle(file_path)\n",
    "    # correct data types\n",
    "    fix_cols = data_df.columns[data_df.dtypes == 'object']\n",
    "    data_df.loc[:, fix_cols] = data_df.loc[:, fix_cols].astype('bool')\n",
    "    # remove redundant features\n",
    "    data_df.drop(['total_review_count', 'halal_review_count', 'halal_negation_count'], inplace=True, axis=1)\n",
    "    # split into features and target\n",
    "    X, y = data_df.drop('halal', axis=1), data_df.halal\n",
    "    return X, y\n",
    "\n",
    "# define resampling method\n",
    "def split_and_resample(X, y, test_size=0, resampling=None):\n",
    "    if test_size > 0:\n",
    "        # setting up testing and training sets\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = X, pd.DataFrame(columns=X.columns, dtype=X.dtypes.values),\\\n",
    "                                            y, pd.Series(name=y.name, dtype=y.dtypes)\n",
    "    # concatenate our training data back together\n",
    "    X = pd.concat([x_train, y_train], axis=1)\n",
    "    # separate minority and majority classes\n",
    "    min_class = X[X.halal==X.halal.value_counts().sort_values().index[0]]\n",
    "    max_class = X[X.halal==X.halal.value_counts().sort_values().index[1]]\n",
    "    if not resampling:\n",
    "        return x_train, y_train, x_test, y_test\n",
    "    # oversampling minority\n",
    "    if resampling == 'oversample':\n",
    "        new_min_class = resample(min_class, replace=True, n_samples=(len(max_class)))\n",
    "        upsampled = pd.concat([max_class, new_min_class])\n",
    "        return upsampled.drop('halal', axis=1), upsampled.halal, x_test, y_test\n",
    "    elif resampling == 'undersample':\n",
    "        new_max_class = resample(max_class, replace=True, n_samples=(len(min_class)))\n",
    "        downsampled = pd.concat([new_max_class, min_class])\n",
    "        return downsampled.drop('halal', axis=1), downsampled.halal, x_test, y_test\n",
    "    elif resampling == 'SMOTE':\n",
    "        x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "    # define scoring metrics\n",
    "    metrics = {'roc_auc': 'roc_auc', 'acc': 'accuracy', 'rec': 'recall'}\n",
    "    # evaluate model\n",
    "    scores = cross_validate(model, X, y, scoring=metrics, cv=cv)\n",
    "    return scores\n",
    "\n",
    "# calculate precision-recall area under curve\n",
    "def pr_auc(y_true, probas_pred):\n",
    "   # calculate precision-recall curve\n",
    "   p, r, _ = precision_recall_curve(y_true, probas_pred)\n",
    "   # calculate area under curve\n",
    "   return auc(r, p)\n",
    "\n",
    "# evaluate a model\n",
    "def pr_auc_score(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "    # define the model evaluation the metric\n",
    "    metric = make_scorer(pr_auc, needs_proba=True)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logistic_reg():\n",
    "    # define the location of the dataset\n",
    "    file_path = '/Users/wesamazaizeh/Desktop/Projects/halal_o_meter/src/features/feature_engineering/restaurant_cat_and_num.pkl'\n",
    "\n",
    "    # load the dataset\n",
    "    X, y = load_dataset(file_path)\n",
    "\n",
    "    # transform bool target to 1/0\n",
    "    y = y*1\n",
    "\n",
    "    # select numerical columns\n",
    "    X_keep = X.columns[X.dtypes == 'float64']\n",
    "    X_num = X.loc[:, X_keep]\n",
    "\n",
    "    # class observations\n",
    "    data = pd.concat([X_num, y], axis=1)\n",
    "    print(data.groupby('halal').mean())\n",
    "\n",
    "    # create dummy features from categorial features\n",
    "    cat_cols = X.columns[X.dtypes == 'bool']\n",
    "    dummy_X = pd.concat([pd.get_dummies(X[col], prefix=col) for col in cat_cols], axis=1)\n",
    "\n",
    "    # drop categorial features and replace with dummy features\n",
    "    X = pd.concat([X, dummy_X], axis=1)\n",
    "    X.drop(cat_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average of percent of halal reviews, count of relevant halal reviews, and count of mentions of halal bacon is higher for halal resaurants than non-halal restaurants as expected.\n",
    "- The average of non-halal relevant reviews and percent of reviews that include halal-negation is higher in non-halal restaurants as expected.\n",
    "- The count of halal-truck mentions doesn't seem significantly different between the two groups. This feature randked 7th in RF so there could be information that's not reflected by the groups averages in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logistic_reg():\n",
    "    # define the location of the dataset\n",
    "    file_path = '/Users/wesamazaizeh/Desktop/Projects/halal_o_meter/src/features/feature_engineering/restaurant_cat_and_num.pkl'\n",
    "    # load the dataset\n",
    "    X, y = load_dataset(file_path)\n",
    "    # transform bool target to 1/0\n",
    "    y = y*1\n",
    "    # select numerical columns\n",
    "    X_keep = X.columns[X.dtypes == 'float64']\n",
    "    X_num = X.loc[:, X_keep]\n",
    "    # class observations\n",
    "    data = pd.concat([X_num, y], axis=1)\n",
    "    # create dummy features from categorial features\n",
    "    cat_cols = X.columns[X.dtypes == 'bool']\n",
    "    dummy_X = pd.concat([pd.get_dummies(X[col], prefix=col) for col in cat_cols], axis=1)\n",
    "    # drop categorial features and replace with dummy features\n",
    "    X = pd.concat([X, dummy_X], axis=1)\n",
    "    X.drop(cat_cols, axis=1, inplace=True)\n",
    "    # split to train and test\n",
    "    X_train, y_train, X_test, y_test = split_and_resample(X, y, test_size=0.2)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesamazaizeh/miniconda3/envs/spacy_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.813 +/- 0.059\n",
      "Accuracy: 0.791 +/- 0.038\n",
      "Recall: 0.928 +/- 0.037\n",
      "Confusion matrix:\n",
      " [[ 15  36]\n",
      " [ 15 102]]\n"
     ]
    }
   ],
   "source": [
    "# load data, convert to proper format and split to train and test\n",
    "X_train, y_train, X_test, y_test = start_logistic_reg()\n",
    "\n",
    "# define default logistic regressor\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# evaluate default logistic regressor\n",
    "cv_scores = evaluate_model(X_train, y_train, logreg)\n",
    "print('AUC-ROC: {:.3f} +/- {:.3f}'.format(cv_scores['test_roc_auc'].mean(), cv_scores['test_roc_auc'].std()))\n",
    "print('Accuracy: {:.3f} +/- {:.3f}'.format(cv_scores['test_acc'].mean(), cv_scores['test_acc'].std()))\n",
    "print('Recall: {:.3f} +/- {:.3f}'.format(cv_scores['test_rec'].mean(), cv_scores['test_rec'].std()))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add class_weight to account for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.807 +/- 0.049\n",
      "Accuracy: 0.688 +/- 0.045\n",
      "Recall: 0.654 +/- 0.072\n",
      "Confusion matrix:\n",
      " [[41  7]\n",
      " [43 77]]\n"
     ]
    }
   ],
   "source": [
    "# load data, convert to proper format and split to train and test\n",
    "X_train, y_train, X_test, y_test = start_logistic_reg()\n",
    "\n",
    "# define balanced logistic regressor\n",
    "logreg_bal = LogisticRegression(class_weight='balanced')\n",
    "logreg_bal.fit(X_train, y_train)\n",
    "y_pred = logreg_bal.predict(X_test)\n",
    "\n",
    "# evaluate balanced model\n",
    "cv_scores = evaluate_model(X_train, y_train, logreg_bal)\n",
    "print('AUC-ROC: {:.3f} +/- {:.3f}'.format(cv_scores['test_roc_auc'].mean(), cv_scores['test_roc_auc'].std()))\n",
    "print('Accuracy: {:.3f} +/- {:.3f}'.format(cv_scores['test_acc'].mean(), cv_scores['test_acc'].std()))\n",
    "print('Recall: {:.3f} +/- {:.3f}'.format(cv_scores['test_rec'].mean(), cv_scores['test_rec'].std()))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set PR-AUC : 0.934 +/- 0.036\n",
      "Accuracy : 0.750\n",
      "Recall : 0.976\n",
      "f1 : 0.852\n",
      "Cofusion matrix:\n",
      " [[  5  39]\n",
      " [  3 121]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.11      0.19        44\n",
      "           1       0.76      0.98      0.85       124\n",
      "\n",
      "    accuracy                           0.75       168\n",
      "   macro avg       0.69      0.54      0.52       168\n",
      "weighted avg       0.72      0.75      0.68       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data, convert to proper format and split to train and test\n",
    "X_train, y_train, X_test, y_test = start_logistic_reg()\n",
    "\n",
    "# define class weights\n",
    "w = {0:26, 1:74}\n",
    "\n",
    "# define custom logistic regression\n",
    "logreg2 = LogisticRegression(class_weight=w, max_iter=1000)\n",
    "logreg2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predict and print performance summary\n",
    "y_pred = logreg2.predict(X_test)\n",
    "PR_auc = pr_auc_score(X_test, y_test, logreg2)\n",
    "print('Train set PR-AUC : {:.3f} +/- {:.3f}'.format(PR_auc.mean(), PR_auc.std()))\n",
    "print('Accuracy : {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Recall : {:.3f}'.format(recall_score(y_test, y_pred)))\n",
    "print('f1 : {:.3f}'.format(f1_score(y_test, y_pred)))\n",
    "print('Cofusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('Classification report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 10\n",
      "Training accuracy: 0.4962630792227205\n",
      "Test accuracy: 0.47023809523809523\n",
      "\n",
      "C: 1\n",
      "Training accuracy: 0.5052316890881914\n",
      "Test accuracy: 0.48214285714285715\n",
      "\n",
      "C: 0.1\n",
      "Training accuracy: 0.6098654708520179\n",
      "Test accuracy: 0.6488095238095238\n",
      "\n",
      "C: 0.001\n",
      "Training accuracy: 0.2750373692077728\n",
      "Test accuracy: 0.24404761904761904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "# Apply the scaler to the test data\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# lasso logistic regression\n",
    "C = [10, 1, .1, .001]\n",
    "\n",
    "for c in C:\n",
    "    model = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    print('C:', c)\n",
    "#     print('Coefficient of each feature:', clf.coef_)\n",
    "    print('Training accuracy:', model.score(X_train_std, y_train))\n",
    "    print('Test accuracy:', model.score(X_test_std, y_test))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spacy_env",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
